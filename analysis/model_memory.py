import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer, BitsAndBytesConfig 
from peft import PeftModel, PeftConfig

# --- 1. ë©”ëª¨ë¦¬ ê³„ì‚° í•¨ìˆ˜ (4-bit ë¡œì§ ë°˜ì˜) ---
def calculate_peft_model_memory(model, is_qlora=False):
    """
    PEFT (LoRA/AdaLoRA/QLoRA) ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¶”ì •í•©ë‹ˆë‹¤.
    """
    total_params = 0
    trainable_params = 0
    non_trainable_params = 0
    
    # ë©”ëª¨ë¦¬ ì¶”ì •ì¹˜ (ë°”ì´íŠ¸/ë§¤ê°œë³€ìˆ˜)
    BYTES_PER_16BIT_PARAM = 2   # 16-bit (LoRA ì–´ëŒ‘í„° ë° 16-bit Base Model)
    BYTES_PER_4BIT_PARAM = 0.5  # 4-bit (QLoRA ì¶”ì •ì¹˜)
    
    trainable_memory_bytes = 0
    base_model_memory_bytes = 0

    print("--- Calculating Layer-wise Memory ---")
    for name, parameter in model.named_parameters():
        num_params = parameter.numel()
        total_params += num_params
        
        if parameter.requires_grad:
            # í•™ìŠµ ê°€ëŠ¥í•œ LoRA/Adapter í…ì„œ (16-bit)
            trainable_params += num_params
            bytes_per_param = BYTES_PER_16BIT_PARAM
            trainable_memory_bytes += num_params * bytes_per_param
            # print(f"LoRA Adapter (Trainable): {name}, Memory: {num_params * bytes_per_param / (1024**2):.2f} MB")
        else:
            # Non-trainable Base Model í…ì„œ
            non_trainable_params += num_params
            
            if is_qlora:
                # 4-bit QLoRA ì¶”ì •ì¹˜ ì ìš© (ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
                bytes_per_param = BYTES_PER_4BIT_PARAM 
            else:
                # ğŸŒŸ 16-bit Base Model ì¶”ì •ì¹˜ ì ìš©
                bytes_per_param = BYTES_PER_16BIT_PARAM
            
            base_model_memory_bytes += num_params * bytes_per_param
            
    total_memory_bytes = trainable_memory_bytes + base_model_memory_bytes
    total_memory_mb = total_memory_bytes / (1024**2)

    memory_mode = '4-bit QLoRA' if is_qlora else '16-bit Full'
    print(f"\n--- Model Weight Memory Summary ({memory_mode} Base Model ê°€ì •) ---")
    print(f"Total Parameters: {total_params:,}")
    print(f"Trainable Parameters (LoRA/Adapter): {trainable_params:,}")
    print(f"Non-trainable Parameters (Base Model): {non_trainable_params:,}")
    print("-" * 30)
    print(f"Trainable Adapter Memory (16-bit): {trainable_memory_bytes / (1024**2):.2f} MB")
    print(f"Base Model Memory ({memory_mode}): {base_model_memory_bytes / (1024**2):.2f} MB")
    print(f"**Estimated Total Model Weight Memory: {total_memory_mb:.2f} MB**")
    
    return total_memory_mb


# ----------------------------------------------------------------------
# ğŸŒŸ 2. ëª¨ë¸ ë¡œë“œ ì„¤ì • (16-bit FP16 ë¡œë”©) ğŸŒŸ
# ----------------------------------------------------------------------
ADAPTER_PATH = "/home/ahnjm/aioptim_adalora/Adaptive-Rank-for-LoRA/outputs_qnli/adalora_small/best"
# ğŸŒŸ BASE_MODEL_NAMEì„ Hugging Face ê²½ë¡œë¡œ ë³€ê²½
BASE_MODEL_NAME = "microsoft/deberta-v3-base"

# QLoRA ì„¤ì • ì œê±°
is_qlora_model = False 
bnb_config = None # 4-bit config ì‚¬ìš© ì•ˆ í•¨


print(f"Loading Base Model: {BASE_MODEL_NAME} with 16-bit (FP16)...")

try:
    # 1. Base Model ë¡œë“œ: 16-bit (FP16)ìœ¼ë¡œ ë¡œë“œ
    base_model = AutoModelForSequenceClassification.from_pretrained(
        BASE_MODEL_NAME,
        # quantization_config=bnb_config, # ì œê±°
        device_map="auto",
        torch_dtype=torch.float16,  # 16-bit ì •ë°€ë„ ì§€ì •
    )

    print(f"Loading PEFT Adapter from: {ADAPTER_PATH}")
    # 2. PEFT Adapter ë¡œë“œ ë° Base Modelì— ê²°í•©
    model = PeftModel.from_pretrained(
        base_model,
        ADAPTER_PATH,
        is_trainable=True # LoRA ëª¨ë¸ ë¡œë“œì‹œ is_trainableì€ ë©”ëª¨ë¦¬ ì¸¡ì •ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤.
    )
    
    model.eval()
    
    print("Model loading successful (FP16). Running memory calculation...")

    # 3. ë©”ëª¨ë¦¬ ê³„ì‚° í•¨ìˆ˜ ì‹¤í–‰ (is_qlora=Falseë¡œ ì „ë‹¬í•˜ì—¬ 16-bit ì¶”ì •ì¹˜ ì‚¬ìš©)
    calculate_peft_model_memory(model, is_qlora=False)

except Exception as e:
    print(f"\nâŒ 16-bit ë¡œë”©ë§ˆì € ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì˜¤ë¥˜: {e}")
    print("ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ ë¬¸ì œ, GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ë˜ëŠ” `BASE_MODEL_NAME`ì´ ì •í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")